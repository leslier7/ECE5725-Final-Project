<!-- Bootstrap core CSS -->
 <!-- 1. write subsections for Design (for the formatting the section) -->


<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Starter Template for Bootstrap</title>

    <!-- Bootstrap core CSS -->
    <link href="dist/css/bootstrap.min.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Playfair+Display:wght@600;700&family=Inter:wght@400;500;600&family=Merriweather:wght@400;500&display=swap" rel="stylesheet">

    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <link href="../../assets/css/ie10-viewport-bug-workaround.css" rel="stylesheet"> -->

    <!-- Custom styles for this template -->
    <link href="starter-template.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy these 2 lines! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->
    <!-- <script src="../../assets/js/ie-emulation-modes-warning.js"></script> -->

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <a class="navbar-brand" href="#">Project name</a>
        </div>
        <div id="navbar" class="collapse navbar-collapse">
          <ul class="nav navbar-nav">
            <li class="active"><a href="#header">Header</a></li>
            <li><a href="#intro">Introduction</a></li>
            <li><a href="#obj">Project Objective</a></li>
            <li><a href="#video">Project Video</a></li>
            <li><a href="#design-testing">Design &amp; Testing</a></li>
            <li><a href="#result">Results</a></li>
            <li><a href="#conclusion">Conclusions</a></li>
            <li><a href="#future">Future Work</a></li>
            <li><a href="#figures">Figures &amp; Media</a></li>
            <li><a href="#team">Team Contributions</a></li>
            <li><a href="#parts">Parts List</a></li>
            <li><a href="#references">References</a></li>
            <li><a href="#code">Code Appendix</a></li>
          </ul>
        </div><!--/.nav-collapse -->
      </div>
    </nav>

    <div class="container page-content">

      <section class="header-panel" id="header">
        <div class="header-panel__intro">
          <h1>Dual-Mode Gesture Control:<br>A Unified Interface for Robot and Game Interaction</h1>
          <p class="lead">ECE5725 Final Project</p>
        </div>
        <div class="header-panel__details">
          <div class="header-details-grid">
            <div class="header-detail">
              <p class="header-detail__label">Team Members</p>
              <ul class="header-detail-list">
                <li>Robbie Leslie (rwl228)</li>
                <li>Shiqi Liu (sl3646)</li>
              </ul>
            </div>
            <div class="header-detail">
              <p class="header-detail__label">Advisor</p>
              <p class="header-detail__value">Dr. Danna Ma</p>
            </div>
            <div class="header-detail">
              <p class="header-detail__label">Submission Date</p>
              <p class="header-detail__value">December 12, 2025</p>
            </div>
            
          </div>
        </div>
      </section>

      <hr id="intro">

      <div style="text-align:center;">
              <h2>Introduction</h2>
              <p style="text-align: left;padding: 0px 30px;">In ECE5725, we explored many ways to use the Raspberry Pi for hardware-software integration, but our interactions with the system remained limited. Most operations relied on keyboards, mice, or prewritten scripts, offering little in terms of intuitive, real-time control.</p>
              <p style="text-align: left;padding: 0px 30px;">In everyday life, our hands are the primary tools for interacting with the physical world—whether driving, eating, gaming, or performing fine-grained tasks. Motivated by this observation, we set out to design a more natural and expressive interface for the Raspberry Pi. Our project introduces a dual-mode unified gesture-control system that allows users to control different applications simply by wearing a sensor-enabled glove.</p>
              <p style="text-align: left;padding: 0px 30px;">We wanted to make sure that both of our modes would demonstrate the strengths of embedded systems. This meant having both a strong graphical component to show the power of the Linux desktop, as well as a hardware component. We decided to build a game and a robot that would utilize our gloves as the control method.</p>
      </div>

    <hr id='obj'>

      <div style="text-align:center;">
          <h2>Project Objective (Will make a new fig)</h2>
      </div>
      <div class="project-objective">
          <div class="project-objective__image">
              <img class="img-rounded" src="pics/systemStructure.png" alt="Generic placeholder image" width="360" height="240">
          </div>
          <div class="project-objective__content">
              <ul class="objective-list">
                  <li>Build a reliable wireless communication channel between the gesture glove and the Raspberry Pi.</li>
                  <li>Stabilize IMU motion data through real-time filtering so the glove behaves more like a controller and less like a jittery sensor.</li>
                  <li>Map hand gestures to continuous motor commands for smooth and intuitive robot navigation.</li>
                  <li>Use the IMU accelerometer to control two cursors in a video game built with C and Raylib.</li>
              </ul>
          </div>
      </div>

    <hr id="video">

      <div class="center-block" style="text-align:center;">
          <h2>Project Video</h2>
          <iframe width="640" height="360" src="https://www.youtube.com/watch?v=et91Gea6CPk" frameborder="0" allowfullscreen></iframe>
          <p style="margin-top:10px;">Demonstration clip highlighting the system in action.</p>
      </div>

    <hr id='design-testing'>

      <div style="text-align:center;">
              <h2>Key Design Milestones</h2>
              <div class="design-sections">
                  <div class="design-section">
                      <h3><span>1</span> Milestone 1 — IMU ↔ Dongle Communication</h3>
                      <p>The first major checkpoint was establishing a reliable data path between the IMU and our wireless dongle. We powered the microcontroller with the IMU over USB-C and connected the dongle to a laptop for rapid debugging. Once we had confirmed that the microcontroller and dongle were communicating correctly, we had to then have the dongle talk to the Pi. This was done using Serial, and we build readers in both C any Python since we used both languages.</p>
                      <p>There were multiple ways that we could have had a microcontroller communicate with the Pi. Two of the most popular are WiFi and Bluetooth. If we chose WiFi, we would have had a microcontroller connect to a network, and then used UDP to steam data packets from the microcontroller to the Pi. This would have been fast enough, and is well documented. However, it would require conencting the microcontroller to a network, and university managed networks are notorious for having limitations which make it difficult to communicate.</p>
                      <p>Bluetooth is a widely used low-power 2.4GHz communication protocol used to stream data. It would have met most of our design requirments, except it can have higher latancy. But the main reason that we choose not to use Bluetooth is ease of development. The Bluetooth protocol is a bit messy, which makes it hard to devleop for. It is well documented, but still can be fiddly.</p>
                      <p>To solve both the latency problem, and the ease of development, we looked into using direct 2.4GHz communication with a dongle. This technique is often used with wireless gaming mice. During our research, we found out about Nordic Semiconductor's Enhanced ShockBurst (ESB). This protocol is used for low latency 2.4GHz communication between one recievcer and up to eight transmitters. It is enabled on the nRF52 series of microcontrollers.</p>
                      <p>And what sealed the deal was the XIAO nRF52840 Sense board with an integrated LSM6DSO 6-axis IMU. Having the IMU built into the microcontroller development board would save us from having two different PCBs attached to the gloves. It also has a built in battery charge circuit, so we would just need to connect a battery and it would be able to charge it over USB-C through the micrcontroller board. Then to connect to the Pi, we could get the nRF52840 Dongle, which is designed for applications like being a reciever over USB. Using this combiation also meant that we got to try using Zephyr, a popular Real-Time Operating System for microcontrollers.</p>
                      <p>Once the microcontroller to Pi communication was complete, Shiqi and Robbie split their development tracks. Shiqi owned the Python-based motor interface, while Robbie moved to the C/Raylib environment required for the slicing game.</p>
                  </div>
                  <div class="design-section">
                      <h3><span>2</span> Milestone 2 — Motor-Control Development</h3>
                      <p><strong>IMU filtering.</strong> Initial experiments used a simple complementary filter to fuse accelerometer and gyroscope readings. While this approach worked for coarse motion, it quickly revealed its limitations during steering: the heading estimate accumulated drift, reacted slowly to quick maneuvers, and lacked the precision needed for smooth gesture control. Since the timeline allowed for further refinement, Shiqi transitioned to a Mahony-based filter. This algorithm provided a more mathematically grounded orientation estimate by incorporating gyroscope feedback and corrective proportional-integral terms. After tuning the filter gains for our specific IMU, the system produced stable, low-noise roll and pitch estimates suitable for continuous real-time control. The improvement was immediately noticeable—tilts felt more responsive, and the robot no longer “wandered off” due to accumulated drift.</p>
                      <p><strong>Gesture-to-motor mapping.</strong> Once reliable orientation estimates were available, Shiqi began designing the mapping from hand gestures to motor behavior. This required assembling the robot chassis, validating the servo power path, and ensuring the Raspberry Pi could generate stable PWM signals. Early mapping attempts used the Lab 3 DC motor system, but the torque was insufficient for smooth directional changes. To improve performance, the team replaced the drivetrain with a Parallax Feedback 360° high-speed servo. The servo offered higher stall torque, consistent rotation speed, and closed-loop position feedback—traits that made real-time gesture-driven control significantly more predictable.</p>
                      <p>With the mechanical system stabilized, the next step involved translating tilt angles into forward and turning commands. Shiqi iterated through several proportional gains and deadband thresholds to prevent jitter while preserving responsiveness. Testing sessions involved holding the glove at various angles, observing wheel response, and adjusting scaling factors until the robot exhibited smooth, intuitive motion that matched hand gestures naturally. The result was a control pipeline that felt direct and continuous: mild tilts produced gentle course corrections, while larger motions triggered sharper turns or acceleration.</p>
                  </div>
                  <div class="design-section">
                      <h3><span>3</span> Milestone 3 — Game Development</h3>
                      <p><strong>Game Template.</strong> To start off developemnt, Robbie chose to use Raylib with C. For the class, we have learned PyGame with Python. Robbie chose to use Raylib and C for a few reasons. The first is a personal preference for C over Python. The second is that he has a bit of experience in Raylib from building a game for a game jam with Raylib and Odin. But the third and most important reason is performance. Python is an interpreted language, which means that is very slow compared to a compiled language like C. Real game developers mostly use C++ to avoid the overhead that comes from inperpreted languages. C was chosen over C++ mostly because of familiarity. To get started, Raylib has an offical C template which contains example code for how to have different screens, and set up a Raylib project. </p>
                      <p><strong>Control Implementation.</strong> With the template in place, the next task was to get the cursors working. This meant reading the IMU data from the dongle, and using it to change the position of the cursor. In order to do that, Robbie developed the algorithm to try and convert the acceleration data into position via double integration. To get proper measurements, the IMUs are calibrated to try and remove any bias they have. Then, we try to filter out gravity so the acceleration is only from movement, this is partially done using a high-pass filter. A deadzone is also applied to get rid of very small movements. Next, the acceleration is integrated to get the velocity. The velocity is damped, clamped, and another deadzone is applied before it is finally integrated to get the position. The results were inconsistent, but good enough to move on to the next parts of development.</p>
                      <p><strong>Gameplay.</strong> With cursors working, it was finally time to add the fruits to the game. Three fruits were added with various sizes, as well as a bomb. The smaller the fruit, the higher the points that were scored for slicing it. Touching a bomb would cause a game over. In order to make testing easier, a debug mode was added where the mouse would act like the left cursor instead of IMU data. With the basic gameplay in place, all that was left was to polish the game by adding buttons, a start and end screen, and the abiltiy to replay the game. A local high score system was also added which keeps track of the best score since the game has been launched. This local high score is only stored in the game, so closing the game resets the high score. We planned on adding a global high score by saving the scores in a file, however this was cut due to time constraints.</p>
                  </div>
              </div>
      </div>

    <hr id='result'>

      <div style="text-align:center;">
              <h2>Results</h2>
              <p style="text-align: left;padding: 0px 30px;">Our system performed reliably during demonstration and met the expectations for both gesture-based robot control and game interaction.</p>
              <p style="text-align: left;padding: 0px 30px;">For the motor-control subsystem, the robot responded accurately to hand tilts, enabling fine-grained directional control and smooth trajectory adjustments. The Mahony filter provided stable orientation estimates suitable for real-time operation, although its convergence time became noticeable during large, sudden orientation changes. These limitations were primarily due to the sensor precision of our IMU and the inherent lag in the filter’s feedback structure. Despite this, the robot achieved consistent responsiveness across trials, proving that our mapping strategy and proportional control logic were effective.</p>
              <p style="text-align: left;padding: 0px 30px;">For the game interaction component, the system successfully translated IMU motion into on-screen cursor movement. However, cursor precision remained limited due to noise from the accelerometer and the difficulty of reconstructing accurate Cartesian positions from orientation alone. While the gameplay remained functional and enjoyable, this highlighted opportunities for algorithmic improvements and potential hardware upgrades.</p>
      </div>

    <hr id='conclusion'>

      <div style="text-align:center;">
              <h2>Conclusions</h2>
              <p style="text-align: left;padding: 0px 30px;">
                  Our final results were able to mostly meet our original design goals. 
                  We successfully demonstrated two distinct interaction modes—robot control 
                  and game control—using low-latency 2.4 GHz communication between the gloves 
                  and the Raspberry Pi. The system proved that a gesture-based interface can 
                  serve as a unified control method for both hardware and software applications.
              </p>

              <p style="text-align: left;padding: 0px 30px;">
                  Through development, we also gained a deeper understanding of the limitations 
                  of 6-axis IMUs. Both the motor-control and cursor-control pipelines would 
                  have benefited from a 9-axis sensor, which would provide an additional 
                  reference vector for more robust orientation reconstruction. While such an 
                  upgrade introduces higher cost and more complex filtering requirements, 
                  the performance improvements would be significant—particularly for reducing 
                  drift and improving responsiveness.
              </p>

              <p style="text-align: left;padding: 0px 30px;">
                  Overall, our prototype validated the feasibility of a dual-mode gesture 
                  interface and highlighted clear paths for future refinement in sensor hardware, 
                  filtering algorithms, and user-experience robustness.
              </p>
      </div>

    <hr id='future'>

      <div style="text-align:center;">
              <h2>Future Work</h2>
              <p style="text-align: left;padding: 0px 30px;">There are several promising directions to extend and improve the current system:</p>
              <p style="text-align: left;padding: 0px 30px;"><strong>Upgrade the IMU hardware.</strong> Our prototype relies on a 6-axis IMU, which constrains orientation reconstruction and cursor precision. Moving to a 9-axis device would provide an additional reference vector, reducing drift and boosting responsiveness for both robot control and game interaction.</p>
              <p style="text-align: left;padding: 0px 30px;"><strong>Reduce control latency in the motor pipeline.</strong> Although the robot responds smoothly, large and rapid hand motions still reveal measurable delay between IMU updates and wheel commands. Streamlining the filtering stack or optimizing the Raspberry Pi control loop could unlock more aggressive tuning and faster driving dynamics.</p>
              <p style="text-align: left;padding: 0px 30px;"><strong>Improve cursor control in the game mode.</strong> With only a 6-axis IMU, reconstructing absolute cursor positions remains noisy. Future revisions could leverage more advanced estimators or higher-grade IMUs to deliver smoother, more precise on-screen motion and a more natural gameplay feel.</p>
              <p style="text-align: left;padding: 0px 30px;"><strong>Add flex-sensor inputs.</strong> The original concept included flex sensors to capture finger gestures. Integrating them would expand the input vocabulary—enabling mode switching, braking, or in-game actions—and make the glove a more versatile controller.</p>
      </div>

    <hr id='figures'>

      <div class="figures-section" style="text-align:center;">
              <h2>Figures, Drawings, and Supporting Media</h2>
              <div class="figure-block">
                <p class="figure-caption">Figure 1. Glove Wiring Schematic</p>
                <img class="img-rounded" src="pics/gloveschematic.png" alt="Glove schematic" style="max-width:50%;height:auto;">
                <p class="figure-desc">Simplified diagram showing the LiPo supply, button with internal pull-up, and the IMU breakout. We reinforced the JST joints with hot glue to keep the connections reliable during demos.</p>
              </div>

              <div class="figure-block">
                <p class="figure-caption">Figure 2. Glove Electronics Layout</p>
                <img class="img-rounded" src="pics/GlovesOnTableMicrocontrollerView.jpg" alt="Glove microcontroller" style="max-width:50%;height:auto;">
                <p class="figure-desc">Each XIAO nRF52840 is mounted on the back of the glove and backed by a Velcro-mounted battery so packs can be swapped quickly without tools.</p>
              </div>

              <div class="figure-block">
                <p class="figure-caption">Figure 3. Button Placement</p>
                <img class="img-rounded" src="pics/GlovesOnTableButtonView.jpg" alt="Glove button" style="max-width:50%;height:auto;">
                <p class="figure-desc">The trigger button is positioned for an easy thumb press. Swappable JST connectors let us disconnect the pack entirely for charging or transport.</p>
              </div>

              <div class="figure-block">
                <p class="figure-caption">Figure 4. Robot Top View</p>
                <img class="img-rounded" src="pics/RobotTopView.jpg" alt="Robot top" style="max-width:50%;height:auto;">
                <p class="figure-desc">Top-down look at the Pi, motor driver, and wiring harness routing. Servos sit farther back to balance the battery weight.</p>
              </div>

              <div class="figure-block">
                <p class="figure-caption">Figure 5. Robot Side View</p>
                <img class="img-rounded" src="pics/motorSideView.png" alt="Robot top" style="max-width:50%;height:auto;">
                <p class="figure-desc">Side look at the Pi, motor driver, and wiring harness routing.</p>
              </div>

              <div class="figure-block">
                <p class="figure-caption">Figure . Fruit Samurai Title Screen</p>
                <img class="img-rounded" src="pics/FruitSamuraiTitle.png" alt="Fruit Samurai title screen" style="max-width:50%;height:auto;">
                <p class="figure-desc">Players start rounds by hovering a cursor over “Start.” The L and R dots correspond to the left and right gloves, so orientation feedback is immediate.</p>
              </div>

              <div class="figure-block">
                <p class="figure-caption">Figure 7. Fruit Samurai Gameplay</p>
                <img class="img-rounded" src="pics/FruitSamuraiGameplay.png" alt="Fruit Samurai gameplay" style="max-width:50%;height:auto;">
                <p class="figure-desc">Live gameplay capture showing simultaneous left/right cursors slicing fruit. Pressing either glove button re-centers both cursors if they drift off-screen.</p>
              </div>
      </div>

    <hr id='team'>

    <div class="row" style="text-align:center;">
          <h2>Team Contributions</h2>
          <p style="text-align: left;padding: 0px 30px;">Each teammate owned a subset of the hardware, firmware, and documentation tasks, and we met daily to integrate and review progress.</p>
          <div style="text-align:center;">
              <img class="img-rounded" src="pics/GroupPhoto.png" alt="Generic placeholder image" style="width:60%;">
              <h4>Project group picture</h4>
          </div>
          <div class="col-md-6 member-card">
              <img class="img-rounded" src="pics/RobbiePhoto.jpg" alt="Robbie Leslie" width="240" height="300">
              <div class="member-card__text">
                  <h3>Robbie Leslie</h3>
                  <p class="member-email">rwl228@cornell.edu</p>
                  <p>Game and Microcontroller Developer</p>
              </div>
          </div>
          <div class="col-md-6 member-card">
              <img class="img-rounded" src="pics/StevenPhoto.png" alt="Shiqi Liu" width="240" height="300">
              <div class="member-card__text">
                  <h3>Shiqi Liu</h3>
                  <p class="member-email">sl3646@cornell.edu</p>
                  <p>Robot Control and IMU System Developer</p>
              </div>
          </div>
      </div>

    <hr id='parts'>
      <div style="font-size:18px">
          <h2>Parts List</h2>
          <ul>
              <li>Raspberry Pi Model 4B with 2GB RAM - $45.00 (included in lab materials)</li>
              <a href="https://www.digikey.com/en/products/detail/seeed-technology-co-ltd/102010469/16652896"><li>2x Seeed Studio XIAO nRF52840 Plus - $31.98</li></a>
              <a href="https://www.digikey.com/en/products/detail/nordic-semiconductor-asa/NRF52840-DONGLE/9491124"><li>nRF52840 Dongle - $10.00</li></a>
              <a href="https://www.adafruit.com/product/2011"><li>2x 3.7V 1200mAh LiPo Battery - $19.90 (found in lab)</li></a>
              <a href="https://www.parallax.com/product/parallax-feedback-360-high-speed-servo/"><li>2x Parallax Feedback 360° High Speed Servo - $55.98 (found in lab)</li></a>
              <li>2x Buttons - Found in Robbie's spare parts drawer</li>
          </ul>
          
          <h3>Total with Pi: $162.86</h3>
          <h3>Total without Pi: $117.86 (with $75.88 provided by lab)</h3>
      </div>
      <hr id='references'>
      <div style="font-size:18px">
          <h2>References</h2>
          <a href="http://www.micropik.com/PDF/SG90Servo.pdf">Tower Pro Servo Datasheet</a><br>
          <a href="http://getbootstrap.com/">Bootstrap</a><br>
          <a href="http://abyz.co.uk/rpi/pigpio/">Pigpio Library</a><br>
          <a href="https://sourceforge.net/p/raspberry-gpio-python/wiki/Home/">R-Pi GPIO Document</a><br>
          <a href="https://www.raylib.com/">Raylib</a><br>
          <a href="https://www.raylib.com/cheatsheet/cheatsheet.html">Raylib Cheatsheet</a><br>
          <a href="https://github.com/raysan5/raylib-game-template">Raylib C Template</a><br>
          <a href="https://wiki.seeedstudio.com/XIAO_BLE/">Seeed Studio XIAO nRF52840 Wiki</a><br>
          <a href="https://docs.nordicsemi.com/bundle/ncs-latest/page/nrf/protocols/esb/index.html">nRF Enhanced ShockBurst Documentation</a><br>
          <a href="https://github.com/nrfconnect/sdk-nrf/tree/v3.1.0/samples">nRF Samples</a><br>
          <a href="https://github.com/nrfconnect/sdk-nrf/tree/v3.1.0/samples/esb">nRF ESB Samples</a><br>
          <a href="https://github.com/zephyrproject-rtos/zephyr/tree/main/samples/sensor/lsm6dso">Zephyr LSM6DSO Example</a><br>
          <a href="https://www.mouser.com/datasheet/2/321/900-00360-Feedback-360-HS-Servo-v1.2-1147206.pdf?srsltid=AfmBOop2REHpS4Hkxb7wehlhGF292uaTMrZgcLZLxUbnWiFKV7wN1jQN">Parallax Feedback 360° High Speed Servo Datasheet</a><br>
          

      </div>

    <hr id='code'>

      <div class="code-appendix" style="text-align:left;">
              <h2>Code Appendix</h2>
              <p><strong>Full source code available on GitHub:</strong> <a href="https://github.com/leslier7/ECE5725-Final-Project/tree/main" target="_blank">https://github.com/leslier7/ECE5725-Final-Project/tree/main</a></p>
              
              <h3>Key Files and Directories</h3>
              <div>
                <ul>
                  <li><strong>microcontroller/</strong> - Firmware for both the glove microcontroller and dongle, as well as old demos. </li>
                  <li><strong>microcontroller/imu_tx</strong> - Firmware for the glove microcontroller. The left and right glove is set with a compiler flag. </li>
                  <li><strong>microcontroller/dongle_rx</strong> - Firmware for the dongle. </li>
                  <li><strong>pi/</strong> - Top level for all the software which runs on the Pi</li>
                  <li><strong>pi/c-game/</strong> - Raylib C implementation of Fruit Samurai game</li>
                  <li><strong>pi/TODO/</strong> - TODO motor control file path</li>
                  <li><strong>website/</strong> - This documentation website</li>
                </ul>
              </div>
      </div>

    </div><!-- /.container -->




    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>
    <script>window.jQuery || document.write('<script src="../../assets/js/vendor/jquery.min.js"><\/script>')</script>
    <script src="dist/js/bootstrap.min.js"></script>
    <!-- IE10 viewport hack for Surface/desktop Windows 8 bug -->
    <!-- <script src="../../assets/js/ie10-viewport-bug-workaround.js"></script> -->
  </body>
</html>
